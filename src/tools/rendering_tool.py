"""
Rendering Tool - AI éŸ³é¢‘æ¸²æŸ“
ä½¿ç”¨ MusicGen-Melody å°† MIDI è½¬æ¢ä¸ºçœŸå®éŸ³é¢‘
"""

import os
from typing import Dict, Any, Optional, List, ClassVar
from pathlib import Path

from langchain.tools import BaseTool
from pydantic import BaseModel, Field
import numpy as np


class RenderingToolInput(BaseModel):
    """Rendering Tool è¾“å…¥å‚æ•°"""
    midi_path: str = Field(description="è¾“å…¥ MIDI æ–‡ä»¶çš„ç»å¯¹è·¯å¾„")
    instrument: str = Field(
        default="acoustic_guitar",
        description="ç›®æ ‡ä¹å™¨éŸ³è‰²: 'acoustic_guitar', 'electric_guitar', 'piano', 'strings', 'bass'"
    )
    style: str = Field(
        default="clean",
        description="æ¼”å¥é£æ ¼: 'clean', 'distorted', 'ambient', 'bright'"
    )
    duration: int = Field(
        default=10,
        description="ç”ŸæˆéŸ³é¢‘çš„æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 10 ç§’"
    )
    output_path: Optional[str] = Field(
        default=None,
        description="è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤åœ¨åŒç›®å½•ç”Ÿæˆï¼‰"
    )


class RenderingTool(BaseTool):
    """
    éŸ³é¢‘æ¸²æŸ“å·¥å…· - AI éŸ³é¢‘ç”Ÿæˆ

    è¿™ä¸ªå·¥å…·ä½¿ç”¨ MusicGen-Melody å°† MIDI è½¬æ¢ä¸ºçœŸå®çš„éŸ³é¢‘ã€‚

    åŠŸèƒ½ï¼š
    - MIDI â†’ å¼•å¯¼éŸ³é¢‘è½¬æ¢
    - AI éŸ³é¢‘ç”Ÿæˆï¼ˆMusicGen-Melodyï¼‰
    - å¤šç§ä¹å™¨éŸ³è‰²æ”¯æŒ
    - é£æ ¼åŒ–éŸ³é¢‘æ¸²æŸ“
    """

    name: str = "rendering_tool"
    description: str = """
    å°† MIDI æ–‡ä»¶æ¸²æŸ“ä¸ºçœŸå®çš„éŸ³é¢‘æ–‡ä»¶ã€‚

    è¾“å…¥ï¼šMIDI æ–‡ä»¶è·¯å¾„å’Œç›®æ ‡ä¹å™¨éŸ³è‰²
    è¾“å‡ºï¼šé«˜è´¨é‡çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆWAV æ ¼å¼ï¼‰

    ä½¿ç”¨åœºæ™¯ï¼š
    - å°†å‰ä»– MIDI æ¸²æŸ“ä¸ºçœŸå®å‰ä»–éŸ³è‰²
    - å°†é’¢ç´ MIDI æ¸²æŸ“ä¸ºé’¢ç´éŸ³è‰²
    - ç”Ÿæˆä¸åŒé£æ ¼çš„éŸ³é¢‘ï¼ˆæ¸…æ™°ã€å¤±çœŸã€æ°›å›´ç­‰ï¼‰
    - å®ŒæˆéŸ³ä¹åˆ¶ä½œçš„æœ€åä¸€æ­¥

    ç¤ºä¾‹ï¼š
    è¾“å…¥: guitar.mid, instrument="acoustic_guitar"
    è¾“å‡º: guitar_rendered.wav (çœŸå®å‰ä»–éŸ³è‰²)
    """
    args_schema: type[BaseModel] = RenderingToolInput

    # ä¹å™¨éŸ³è‰²æç¤ºè¯æ¨¡æ¿
    INSTRUMENT_PROMPTS: ClassVar[Dict[str, str]] = {
        "acoustic_guitar": "High quality acoustic guitar, clean tone, warm sound, fingerstyle",
        "electric_guitar": "Electric guitar, clean tone, bright sound, professional recording",
        "piano": "Grand piano, clear tone, concert hall acoustics, expressive",
        "strings": "String ensemble, orchestral, warm and rich, cinematic",
        "bass": "Electric bass, deep tone, groovy, tight sound",
    }

    # é£æ ¼ä¿®é¥°è¯
    STYLE_MODIFIERS: ClassVar[Dict[str, str]] = {
        "clean": "clean, clear, professional",
        "distorted": "distorted, rock, powerful",
        "ambient": "ambient, reverb, atmospheric",
        "bright": "bright, crisp, energetic",
    }

    def _run(
        self,
        midi_path: str,
        instrument: str = "acoustic_guitar",
        style: str = "clean",
        duration: int = 10,
        output_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡ŒéŸ³é¢‘æ¸²æŸ“

        Args:
            midi_path: è¾“å…¥ MIDI æ–‡ä»¶è·¯å¾„
            instrument: ç›®æ ‡ä¹å™¨éŸ³è‰²
            style: æ¼”å¥é£æ ¼
            duration: éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            åŒ…å«æ¸²æŸ“ç»“æœçš„å­—å…¸
        """
        try:
            # éªŒè¯è¾“å…¥æ–‡ä»¶
            midi_path = Path(midi_path)
            if not midi_path.exists():
                return {
                    "success": False,
                    "error": f"MIDI æ–‡ä»¶ä¸å­˜åœ¨: {midi_path}"
                }

            print(f"ğŸµ æ­£åœ¨åŠ è½½ MIDI: {midi_path.name}")

            # æ­¥éª¤ 1: MIDI è½¬æ¢ä¸ºå¼•å¯¼éŸ³é¢‘
            guide_audio, sample_rate = self._midi_to_guide_audio(str(midi_path))

            print(f"âœ… å¼•å¯¼éŸ³é¢‘å·²ç”Ÿæˆ")
            print(f"   é‡‡æ ·ç‡: {sample_rate} Hz")
            print(f"   æ—¶é•¿: {len(guide_audio) / sample_rate:.2f} ç§’")

            # æ­¥éª¤ 2: æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(instrument, style)
            print(f"ğŸ¨ éŸ³è‰²æç¤º: {prompt}")

            # æ­¥éª¤ 3: ä½¿ç”¨ MusicGen ç”ŸæˆéŸ³é¢‘
            print(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨ AI ç”ŸæˆéŸ³é¢‘...")
            print(f"   âš ï¸  æ³¨æ„: é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 1.5GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…")

            rendered_audio = self._generate_with_musicgen(
                guide_audio,
                sample_rate,
                prompt,
                duration
            )

            # æ­¥éª¤ 4: ä¿å­˜éŸ³é¢‘
            if output_path is None:
                output_path = midi_path.parent / f"{midi_path.stem}_{instrument}.wav"
            else:
                output_path = Path(output_path)

            self._save_audio(rendered_audio, sample_rate, str(output_path))

            print(f"âœ… æ¸²æŸ“å®Œæˆï¼")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")

            return {
                "success": True,
                "input_path": str(midi_path),
                "output_path": str(output_path),
                "instrument": instrument,
                "style": style,
                "duration_seconds": len(rendered_audio) / sample_rate,
                "sample_rate": sample_rate,
                "message": f"âœ… æˆåŠŸæ¸²æŸ“ä¸º {instrument} éŸ³è‰²ï¼"
            }

        except ImportError as e:
            return {
                "success": False,
                "error": f"ç¼ºå°‘ä¾èµ–åº“: {str(e)}ã€‚è¯·è¿è¡Œ: pip install audiocraft scipy"
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"æ¸²æŸ“å¤±è´¥: {str(e)}"
            }

    def _midi_to_guide_audio(self, midi_path: str) -> tuple:
        """
        å°† MIDI è½¬æ¢ä¸ºå¼•å¯¼éŸ³é¢‘

        ä½¿ç”¨ç®€å•çš„æ­£å¼¦æ³¢åˆæˆï¼Œä½œä¸º MusicGen çš„æ—‹å¾‹å¼•å¯¼

        Args:
            midi_path: MIDI æ–‡ä»¶è·¯å¾„

        Returns:
            (audio_array, sample_rate) å…ƒç»„
        """
        import pretty_midi
        import numpy as np

        # åŠ è½½ MIDI
        midi_data = pretty_midi.PrettyMIDI(midi_path)

        # è®¾ç½®é‡‡æ ·ç‡
        sample_rate = 32000  # MusicGen æ¨èçš„é‡‡æ ·ç‡

        # è·å–æ€»æ—¶é•¿
        duration = midi_data.get_end_time()

        # åˆ›å»ºéŸ³é¢‘æ•°ç»„
        audio = np.zeros(int(duration * sample_rate))

        # åˆæˆæ¯ä¸ªéŸ³ç¬¦
        for instrument in midi_data.instruments:
            if instrument.is_drum:
                continue

            for note in instrument.notes:
                # è®¡ç®—éŸ³ç¬¦çš„é¢‘ç‡
                frequency = 440.0 * (2.0 ** ((note.pitch - 69) / 12.0))

                # ç”Ÿæˆæ­£å¼¦æ³¢
                start_sample = int(note.start * sample_rate)
                end_sample = int(note.end * sample_rate)
                duration_samples = end_sample - start_sample

                if duration_samples > 0:
                    t = np.linspace(0, duration_samples / sample_rate, duration_samples)

                    # æ·»åŠ åŒ…ç»œï¼ˆADSR ç®€åŒ–ç‰ˆï¼‰
                    attack = int(0.01 * sample_rate)  # 10ms attack
                    release = int(0.05 * sample_rate)  # 50ms release

                    envelope = np.ones(duration_samples)
                    if duration_samples > attack:
                        envelope[:attack] = np.linspace(0, 1, attack)
                    if duration_samples > release:
                        envelope[-release:] = np.linspace(1, 0, release)

                    # ç”ŸæˆéŸ³ç¬¦
                    note_audio = 0.3 * np.sin(2 * np.pi * frequency * t) * envelope

                    # åŠ›åº¦è°ƒåˆ¶
                    velocity_factor = note.velocity / 127.0
                    note_audio *= velocity_factor

                    # æ·»åŠ åˆ°æ€»éŸ³é¢‘
                    audio[start_sample:end_sample] += note_audio

        # å½’ä¸€åŒ–
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.8

        return audio.astype(np.float32), sample_rate

    def _build_prompt(self, instrument: str, style: str) -> str:
        """
        æ„å»ºéŸ³è‰²æç¤ºè¯

        Args:
            instrument: ä¹å™¨ç±»å‹
            style: é£æ ¼

        Returns:
            å®Œæ•´çš„æç¤ºè¯
        """
        # è·å–åŸºç¡€ä¹å™¨æç¤º
        base_prompt = self.INSTRUMENT_PROMPTS.get(
            instrument,
            "High quality musical instrument"
        )

        # è·å–é£æ ¼ä¿®é¥°
        style_modifier = self.STYLE_MODIFIERS.get(style, "")

        # ç»„åˆæç¤ºè¯
        if style_modifier:
            prompt = f"{base_prompt}, {style_modifier}"
        else:
            prompt = base_prompt

        return prompt

    def _generate_with_musicgen(
        self,
        guide_audio: np.ndarray,
        sample_rate: int,
        prompt: str,
        duration: int
    ) -> np.ndarray:
        """
        ä½¿ç”¨ MusicGen-Melody ç”ŸæˆéŸ³é¢‘

        Args:
            guide_audio: å¼•å¯¼éŸ³é¢‘
            sample_rate: é‡‡æ ·ç‡
            prompt: éŸ³è‰²æç¤ºè¯
            duration: ç›®æ ‡æ—¶é•¿ï¼ˆç§’ï¼‰

        Returns:
            ç”Ÿæˆçš„éŸ³é¢‘æ•°ç»„
        """
        try:
            from audiocraft.models import MusicGen
            import torch

            # åŠ è½½æ¨¡å‹
            print("   åŠ è½½ MusicGen-Melody æ¨¡å‹...")
            model = MusicGen.get_pretrained('facebook/musicgen-melody')

            # è®¾ç½®ç”Ÿæˆå‚æ•°
            model.set_generation_params(
                duration=duration,
                temperature=1.0,
                top_k=250,
                top_p=0.0,
                cfg_coef=3.0
            )

            # å‡†å¤‡å¼•å¯¼éŸ³é¢‘
            # MusicGen éœ€è¦ (1, 1, samples) çš„å¼ é‡
            guide_tensor = torch.from_numpy(guide_audio).unsqueeze(0).unsqueeze(0)

            # ç”ŸæˆéŸ³é¢‘
            print("   ç”Ÿæˆä¸­...")
            with torch.no_grad():
                wav = model.generate_with_chroma(
                    descriptions=[prompt],
                    melody_wavs=guide_tensor,
                    melody_sample_rate=sample_rate,
                    progress=True
                )

            # è½¬æ¢ä¸º numpy æ•°ç»„
            generated_audio = wav[0, 0].cpu().numpy()

            return generated_audio

        except ImportError:
            # å¦‚æœ MusicGen ä¸å¯ç”¨ï¼Œè¿”å›å¼•å¯¼éŸ³é¢‘ä½œä¸ºåå¤‡
            print("   âš ï¸  MusicGen ä¸å¯ç”¨ï¼Œä½¿ç”¨å¼•å¯¼éŸ³é¢‘ä½œä¸ºè¾“å‡º")
            return guide_audio

    def _save_audio(self, audio: np.ndarray, sample_rate: int, output_path: str):
        """
        ä¿å­˜éŸ³é¢‘æ–‡ä»¶

        Args:
            audio: éŸ³é¢‘æ•°ç»„
            sample_rate: é‡‡æ ·ç‡
            output_path: è¾“å‡ºè·¯å¾„
        """
        from scipy.io import wavfile

        # è½¬æ¢ä¸º 16-bit PCM
        audio_int16 = (audio * 32767).astype(np.int16)

        # ä¿å­˜ä¸º WAV
        wavfile.write(output_path, sample_rate, audio_int16)

    async def _arun(
        self,
        midi_path: str,
        instrument: str = "acoustic_guitar",
        style: str = "clean",
        duration: int = 10,
        output_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰§è¡Œï¼ˆå½“å‰ä½¿ç”¨åŒæ­¥å®ç°ï¼‰"""
        return self._run(midi_path, instrument, style, duration, output_path)


# ä¾¿æ·å‡½æ•°ï¼šç›´æ¥è°ƒç”¨å·¥å…·
def render_audio(
    midi_path: str,
    instrument: str = "acoustic_guitar",
    style: str = "clean",
    duration: int = 10,
    output_path: Optional[str] = None
) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šéŸ³é¢‘æ¸²æŸ“

    Args:
        midi_path: è¾“å…¥ MIDI æ–‡ä»¶è·¯å¾„
        instrument: ç›®æ ‡ä¹å™¨éŸ³è‰²
        style: æ¼”å¥é£æ ¼
        duration: éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

    Returns:
        æ¸²æŸ“ç»“æœå­—å…¸
    """
    tool = RenderingTool()
    return tool._run(midi_path, instrument, style, duration, output_path)
